= About RHCOS

Red Hat Enterprise Linux CoreOS (RHCOS) represents the next generation of single-purpose
container operating system technology. Created by the same development teams
that created Red Hat Enterprise Linux Atomic Host and CoreOS Container Linux,
RHCOS combines the quality standards of Red Hat Enterprise Linux (RHEL)
with the automated, remote upgrade features from Container Linux.

RHCOS is supported only as a component of OpenShift Container Platform for all OpenShift Container Platform machines. 
RHCOS is the only supported operating system for OpenShift Container Platform control plane, or master,
machines. While RHCOS is the default operating system for all cluster
machines, you can create compute machines, which are also known as worker machines, that use RHEL as their
operating system. There are two general ways RHCOS is deployed in
OpenShift Container Platform:

* If you install your cluster on infrastructure that the cluster provisions,
RHCOS images are downloaded to the target platform during installation,
and suitable Ignition config files, which control the RHCOS configuration,
are used to deploy the machines.

* If you install your cluster on infrastructure
that you manage, you must follow the installation documentation to obtain the
RHCOS images, generate Ignition config files, and use the Ignition config
files to provision your machines.

== Key RHCOS features

The following list describes key features of the RHCOS operating system:

* **Based on RHEL**: The underlying operating system consists primarily of RHEL components.
The same quality, security, and control measures that support RHEL also support
RHCOS. For example, RHCOS software is in
RPM packages, and each RHCOS system starts up with a RHEL kernel and a set
of services that are managed by the systemd init system.

* **Controlled immutability**: Although it contains RHEL components, RHCOS
is designed to be managed
more tightly than a default RHEL installation. Management is
performed remotely from the OpenShift Container Platform cluster. When you set up your
RHCOS machines, you can modify only a few system settings. This controlled
immutability allows OpenShift Container Platform to
store the latest state of RHCOS systems in the cluster so it is always
able to create additional machines and perform updates based on the latest RHCOS
configurations.

* **CRI-O container runtime**: Although RHCOS contains features for running the
OCI- and libcontainer-formatted containers that Docker requires, it incorporates
the CRI-O container engine
instead of the Docker container engine. By focusing on features needed by
Kubernetes platforms, such as OpenShift Container Platform, CRI-O can offer specific
compatibility with different Kubernetes versions. CRI-O also offers a smaller
footprint and reduced attack surface than is possible with container engines
that offer a larger feature set. At the moment, CRI-O is only available as a
container engine within OpenShift Container Platform clusters.

* **Set of container tools**: For tasks such as building, copying, and otherwise
managing containers, RHCOS replaces the Docker CLI tool with a compatible
set of container tools. The podman CLI tool supports many container runtime
features, such as running, starting, stopping, listing, and removing containers
and container images. The skopeo CLI tool can copy, authenticate, and sign
images. You can use the crictl CLI tool to work with containers and pods from the
CRI-O container engine. While direct use of these tools in RHCOS is
discouraged, you can use them for debugging purposes.

* **rpm-ostree upgrades**: RHCOS features transactional upgrades using
the `rpm-ostree` system.
Updates are delivered by means of container images and are part of the
OpenShift Container Platform update process. When deployed, the container image is pulled,
extracted, and written to disk, then the bootloader is modified to boot into
the new version. The machine will reboot into the update in a rolling manner to
ensure cluster capacity is minimally impacted.

* **Updated through MachineConfigOperator**:
In OpenShift Container Platform, the Machine Config Operator handles operating system upgrades.
Instead of upgrading individual packages, as is done with `yum`
upgrades, `rpm-ostree` delivers upgrades of the OS as an atomic unit. The
new OS deployment is staged during upgrades and goes into effect on the next reboot.
If something goes wrong with the upgrade, a single rollback and reboot returns the
system to the previous state. RHCOS upgrades in OpenShift Container Platform are performed
during cluster updates.

For RHCOS systems, the layout of the `rpm-ostree` file system has the
 following characteristics:

* `/usr` is where the operating system binaries and libraries are stored and is
 read-only. We do not support altering this.
* `/etc`, `/boot`, `/var` are writable on the system but only intended to be altered
 by the Machine Config Operator.
* `/var/lib/containers` is the graph storage location for storing container
 images.


== Choosing how to configure RHCOS

RHCOS is designed to deploy on an OpenShift Container Platform cluster with a minimal amount
of user configuration. In its most basic form, this consists of:

* Starting with a provisioned infrastructure, such as on AWS, or provisioning
the infrastructure yourself.

* Supplying a few pieces of information, such as credentials and cluster name,
in an `install-config.yaml` file when running `openshift-install`.

Because RHCOS systems in OpenShift Container Platform are designed to be fully managed
from the OpenShift Container Platform cluster after that, directly changing an RHCOS machine is
discouraged. Although limited direct access to RHCOS machines
cluster can be accomplished for debugging purposes, *you should not directly configure
RHCOS systems*.
Instead, if you need to add or change features on your OpenShift Container Platform nodes,
consider making changes in the following ways:

* **Kubernetes workload objects (DaemonSets, Deployments, etc.)**: If you need to
add services or other user-level features to your cluster, consider adding them as
Kubernetes workload objects. Keeping those features outside of specific node
configurations is the best way to reduce the risk of breaking the cluster on
subsequent upgrades.

* **Day-2 customizations**: If possible, bring up a cluster without making any
customizations to cluster nodes and make necessary node changes after the cluster is up.
Those changes are easier to track later and less likely to break updates.
Creating MachineConfigs or modifying Operator custom resources
are ways of making these customizations.

* **Day-1 customizations**: For customizations that you must implement when the
cluster first comes up, there are ways of modifying your cluster so changes are
implemented on first boot.
Day-1 customizations can be done through Ignition configs and manifest files
during `openshift-install` or by adding boot options during ISO installs
provisioned by the user.

Here are examples of customizations you could do on day-1:

* **Kernel arguments**: If particular kernel features or tuning is needed on nodes when the cluster first boots.

* **Disk encryption**: If your security needs require that the root filesystem on the nodes are encrypted, such as with FIPS support.

* **Kernel modules**: If a particular hardware device, such as a network card or video card, does not have a usable module available by default in the Linux kernel.

* **Chronyd**: If you want to provide specific clock settings to your nodes,
such as the location of time servers.

To accomplish these tasks, you can augment the `openshift-install` process to include additional
objects such as MachineConfigs.
Those procedures that result in creating MachineConfigs can be passed to the Machine Config Operator
after the cluster is up.


[NOTE]
====
The Ignition config files that the installation program generates contain certificates that expire after 24 hours. You must complete your cluster installation and keep the cluster running for 24 hours in a non-degraded state to ensure that the first certificate rotation has finished.
====

== Choosing how to configure RHCOS

Differences between RHCOS deployments for OpenShift Container Platform are based on
whether you are deploying on an infrastructure provisioned by the installer or by the user:

* **Installer provisioned**: Some cloud environments offer pre-configured infrastructures
that allow you to bring up an OpenShift Container Platform cluster with minimal configuration.
For these types of deployments, you can supply Ignition configs
that place content on each node so it is there when the cluster first boots.

* **User provisioned**: If you are provisioning your own infrastructure, you have more flexibility
in how you add content to a RHCOS node. For example, you could add kernel
arguments when you boot the RHCOS ISO installer to install each system.
However, in most cases where configuration is required on the operating system
itself, it is best to provide that configuration through an Ignition config.

The Ignition facility runs only when the RHCOS system is first set up.
After that, Ignition configs can be supplied later using the MachineConfigs.